{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Phishing Detection Using BERT- Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents \n",
    "* Introduction\n",
    "* Required Libraries\n",
    "* Dataset\n",
    "* Downloading pretrained model\n",
    "* Training\n",
    "* Evaluation\n",
    "* Conclusion\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Phishing is used by malicous actors to obtain sensitive information from email users by pretending to be from legitimate institutions/people. Traditional methods are rigid and reactive. They rely on keyword matching and previously seen malicous URLs to detect phishing emails. By using a language model to infernece on an whole email message, we build a more robust model that utlizes the entire context of an email and generalizes to previously unseen messages.\n",
    "In this notebook, we show how to train a [BERT](https://arxiv.org/pdf/1810.04805.pdf) transformer language model and analyse the performance on an example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "from cudf.core.subword_tokenizer import SubwordTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import requests\n",
    "import os.path\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from morpheus.utils.seed import manual_seed\n",
    "\n",
    "from common.sequence_classifier import SequenceClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Due to the limited public availability of labeled email datasets, for this example we are using the labeled SMS Spam Collection Data Set from the UCI Machine Learning Repository. \n",
    "SMSSPAM contain deceptive information, some of the messages have the intent of convincing the recipient to give the sender money or to share information.\n",
    "\n",
    "* [SMSSPAM](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Downloading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"smsspamcollection.zip\"):    \n",
    "    URL = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "    response = requests.get(URL)\n",
    "    open(\"smsspamcollection.zip\", \"wb\").write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab53f9571d479ee677e7b283a06a661a  smsspamcollection.zip\n"
     ]
    }
   ],
   "source": [
    "!md5sum smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if you have the same version with the checksum we got when we ran the notebook: ab53f9571d479ee677e7b283a06a661a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"SMSSpamCollection\"):\n",
    "    !unzip smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = cudf.read_csv(\"SMSSpamCollection\", delimiter='\\t', header=None, names=['spam/ham', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert label to binary 0 = ham, 1 = spam\n",
    "df[\"label\"] = df[\"spam/ham\"].str.match('spam').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into training (80%) and test (20%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[\"message\"], df[\"label\"], train_size=0.8,random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize/Load BERT model\n",
    "Load the pre-trained bert-base-uncased model from [Hugging Face](https://huggingface.co/bert-base-uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seq_classifier = SequenceClassifier(\"bert-base-uncased\", hash_file=\"../../../morpheus/data/bert-base-uncased-hash.txt\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seeds for model reproducability\n",
    "manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 1/2 [00:33<00:33, 33.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08998454025214804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 2/2 [01:08<00:00, 34.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.015057251792833475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seq_classifier.train_model(X_train, y_train, batch_size=32, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save as pytorch model\n",
    "torch.save(seq_classifier._model, \"phishing-bert.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99375"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_classifier.evaluate_model(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/morpheus/lib/python3.10/site-packages/cudf/core/indexed_frame.py:3424: FutureWarning: The append method is deprecated and will be removed in a future version. Use cudf.concat instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_preds = seq_classifier.predict(X_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9767441860465117"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = test_preds.to_numpy()\n",
    "true_labels = y_test.to_numpy()\n",
    "f1_score(true_labels, tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Export Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = SubwordTokenizer(\"../../../morpheus/data/bert-base-uncased-hash.txt\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer_output = tokenizer(df[\"message\"][0:3],\n",
    "                                max_length=128,\n",
    "                                max_num_rows=3,\n",
    "                                truncation=True,\n",
    "                                add_special_tokens=True,\n",
    "                                return_tensors=\"pt\")\n",
    "\n",
    "sample_model_input = (tokenizer_output[\"input_ids\"].type(torch.long), tokenizer_output[\"attention_mask\"].type(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.onnx.export(seq_classifier._model,              \n",
    "                  sample_model_input,               \n",
    "                  \"model.onnx\",                                      # where to save the model\n",
    "                  export_params=True,                                # store the trained parameter weights inside the model file\n",
    "                  opset_version=10,                                  # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,                          # whether to execute constant folding for optimization\n",
    "                  input_names = ['input_ids','attention_mask'],      # the model's input names\n",
    "                  output_names = ['output'],                         # the model's output names\n",
    "                  dynamic_axes={'input_ids' : {0 : 'batch_size'},    # variable length axes\n",
    "                                'attention_mask': {0: 'batch_size'}, \n",
    "                                'output' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show that using a BERT-based spam/phishing detector performs well in identifying the spam messages across this dataset with an F1 score above 0.95. This notebook is prepared as an example. We have seen an equally strong performance using private datasets with bengin and phishing emails; and we suggest users experiment with their own datasets as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* SMS Dataset https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "\n",
    "* SMS Dataset http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/\n",
    "\n",
    "* BERT model hosted on HuggingFace https://huggingface.co/bert-base-uncased\n",
    "\n",
    "* Spam Detection Using BERT - Thaer Sahmoud, Dr. Mohammad Mikki (2022) https://arxiv.org/abs/2206.02443\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of BERT Fine-Tuning Sentence Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
