{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGX Kernel Root Cause Analysis Acceleration & Predictive Maintenance using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents \n",
    "* Introduction\n",
    "* Dataset\n",
    "* Reading in the datasets\n",
    "* Initialize/Load BERT model\n",
    "* Training - DGX Kernel logs dataset\n",
    "* Evaluation\n",
    "* Conclusion\n",
    "* References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Like any other Linux based machine, DGX's generate a vast amount of logs. Analysts spend hours trying to identify the root causes of each failure. There could be infinitely many types of root causes of the failures. Some patterns might help to narrow it down; however, regular expressions can only help to identify previously known patterns. Moreover, this creates another manual task of maintaining a search script. \n",
    "\n",
    "In this notebook, we show how GPU's can accelerate the analysis of the enormous amount of logs using machine learning. Another benefit of analyzing in a probabilistic way is that we can pin down previously undetected root causes. To achieve this, we will fine-tune a pre-trained BERT* model with a classification layer using HuggingFace library.\n",
    "\n",
    "Once the model is capable of identifying even the new root causes, it could also be deployed as a process running in the machines to predict failures before they happen.\n",
    "\n",
    "*BERT stands for Bidirectional Encoder Representations from Transformers. The paper can be found [here.](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "* DGX Linux Kernel logs\n",
    "\n",
    "The dataset comprises `kern.log` files from multiple DGX's. Each line inside has been labelled as either `0` for `ordinary` or `1` for `root cause` by a script that uses some known patterns. We will be especially interested in lines that are marked as ordinary in the test set but predicted as a root cause, as they may be new types of root causes of failures.\n",
    "\n",
    "More information on Linux log types can be found [here.](https://help.ubuntu.com/community/LinuxLogFiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cudf;\n",
    "from binary_sequence_classifier import BinarySequenceClassifier;\n",
    "from os import path;\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from morpheus.utils.seed import manual_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_UkeC7SG2krJ"
   },
   "outputs": [],
   "source": [
    "dflogs = pd.read_csv(\"../../datasets/training-data/root-cause-training-data.csv\",header=None,names=[\"label\",\"log\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the `log` column has a line from the `kern.log` file, and the label column has information on whether it is an ordinary log or a root cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously unseen error types are read into a new dataframe to be appended to the test set later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnewerror=pd.read_csv(\"../../datasets/training-data/root-cause-unseen-errors.csv\", header=None, names=['label', 'log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dflogs, dflogs.label,random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=pd.concat([X_test,dfnewerror])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=pd.concat([y_test,dfnewerror[\"label\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.to_csv(\"Rootcause-training-data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.to_csv(\"Rootcause-validation-data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_json(\"Rootcause-training-data.jsonlines\", orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_json(\"Rootcause-validation-data.jsonlines\",orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnewerror=cudf.DataFrame.from_pandas(dfnewerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflogs=cudf.DataFrame.from_pandas(dflogs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize/Load the BERT model\n",
    "We will initialize the sequence classifier module with a pre-trained BERT model. The pre-trained model we use is located at https://huggingface.co/bert-base-uncased For more information on the model, please see the paper at https://arxiv.org/pdf/2005.01634.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "seq_classifier = BinarySequenceClassifier()\n",
    "seq_classifier.init_model(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the dataset will be used for fine-tuning the model. The rest of the dataset will be used as the test set to evaluate if the model is useful. With default settings, 80% of the dataset will be the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of epochs should be adjusted for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 1/1 [00:08<00:00,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6523606370795857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "manual_seed(random_seed)\n",
    "seq_classifier.train_model(X_train[\"log\"], y_train,batch_size=128, epochs=1,learning_rate=3.6e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "seq_classifier.save_model(timestr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`evaluate_model` returns the accuracy in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check if any of the error lines we appended are predicted as root-cause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9601666666666666"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_classifier.evaluate_model(X_test[\"log\"], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the predictions from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = seq_classifier.predict(X_test[\"log\"], batch_size=128, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = test_preds[0].to_numpy()\n",
    "true_labels = X_test[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the F1 score since it's not a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9765765765765766"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(true_labels, tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is higher than the F1 score. The distribution of the labels is not balanced hence accuracy might be less indicative of performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a confusion matrix to check how many of each label are predicted as marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[189,  13],\n",
       "       [  0, 271]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(true_labels, tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpredseries=(test_preds[0]).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   1,   7,   9,  11,  12,  13,  14,  16,  18,  19,  20,  22,\n",
       "         23,  24,  25,  29,  31,  34,  42,  44,  45,  46,  47,  49,  50,\n",
       "         51,  52,  53,  55,  56,  57,  58,  59,  61,  62,  63,  64,  65,\n",
       "         66,  67,  68,  69,  70,  71,  73,  74,  75,  80,  81,  82,  85,\n",
       "         86,  88,  89,  90,  92,  93,  94,  95,  97,  98,  99, 101, 103,\n",
       "        104, 105, 106, 107, 109, 110, 112, 113, 114, 115, 117, 119, 120,\n",
       "        123, 124, 125, 126, 128, 129, 130, 131, 134, 136, 137, 138, 139,\n",
       "        140, 141, 142, 143, 144, 145, 146, 148, 149, 150, 153, 154, 156,\n",
       "        159, 160, 161, 162, 163, 164, 165, 166, 170, 172, 173, 174, 175,\n",
       "        176, 178, 181, 182, 183, 187, 188, 191, 194, 196, 200, 201, 202,\n",
       "        205, 206, 207, 209, 210, 214, 215, 216, 217, 218, 221, 222, 223,\n",
       "        224, 226, 227, 228, 229, 231, 234, 235, 236, 237, 240, 244, 245,\n",
       "        246, 250, 251, 254, 255, 256, 257, 262, 265, 266, 267, 268, 269,\n",
       "        271, 274, 275, 276, 278, 284, 285, 287, 289, 290, 291, 292, 293,\n",
       "        294, 295, 296, 299, 300, 301, 302, 303, 304, 309, 311, 312, 315,\n",
       "        316, 317, 318, 319, 321, 322, 323, 328, 329, 330, 331, 332, 334,\n",
       "        335, 340, 343, 347, 349, 350, 351, 352, 353, 354, 355, 356, 357,\n",
       "        358, 361, 363, 365, 367, 369, 373, 374, 375, 378, 379, 380, 381,\n",
       "        382, 383, 384, 385, 386, 388, 389, 390, 392, 394, 395, 396, 397,\n",
       "        398, 399, 400, 401, 402, 404, 406, 407, 408, 409, 410, 411, 412,\n",
       "        414, 415, 416, 418, 419, 420, 421, 425, 426, 429, 432, 434, 435,\n",
       "        436, 437, 438, 441, 442, 443, 444, 450, 453, 460, 471]),)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.where(testpredseries == 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we didn't include hardware errors in the training set, the model still marks some of them as root cause lines which can be seen from the indices of the positive predictions above. The lines identified by this model may indicate a problem hours before the failure or outage. This approach can be implemented on the machines to warn the users well before the problems occur so corrective actions can be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "* https://github.com/huggingface/transformers/tree/master/examples#\n",
    "* BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding https://arxiv.org/pdf/1810.04805.pdf"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of BERT Fine-Tuning Sentence Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
